{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d3af0c-5813-4e51-9cdc-fd083a72b649",
   "metadata": {},
   "source": [
    "# A2 GPT-2\n",
    "\n",
    "This is an example of how to apply GPT-2, specifically the distilgpt2 model using `GPT2ForSequenceClassification`, to the 3-way sentiment analysis task from assignment 2. Much of this code is copied from the BERT example (`a2climatesentimentGPT.ipynb`).\n",
    "\n",
    "Again, as for the BERT example, there is some variation in terms of results each time this is run, but results on the test set are usually better than all methods considered in assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10134717-bc80-41c1-a2f7-561b122c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_texts_and_labels(fname):\n",
    "    csv_reader = csv.reader(open(fname))\n",
    "    # Ignore header row\n",
    "    next(csv_reader)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for line in csv_reader:\n",
    "        id,text,label = line\n",
    "        label = int(label)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return texts,labels\n",
    "\n",
    "train_texts, train_labels = get_texts_and_labels('data/train-sample.csv')\n",
    "val_texts, val_labels = get_texts_and_labels('data/dev.csv')\n",
    "test_texts,test_labels = get_texts_and_labels('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5861a76-9595-4c76-8398-67c37f7a40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe698ee6-2397-4d76-b9f7-f934b207d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class A2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = A2Dataset(train_encodings, train_labels)\n",
    "val_dataset = A2Dataset(val_encodings, val_labels)\n",
    "test_dataset = A2Dataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfb439-57cd-44d7-a4ac-8ee9c9918999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    #num_train_epochs=3,              # total number of training epochs\n",
    ")\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"distilgpt2\", num_labels=3)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10c776-0fdd-482f-8eae-78fcc5e3052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_predicted_labels = np.argmax(test_predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f006372-cd69-4332-9daf-a52588e4ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# A helper function to print out macro averaged P,R, and F1 and accuracy.\n",
    "# Uses implementantions of evaluation metrics from sklearn.\n",
    "def print_results(gold_labels, predicted_labels):\n",
    "    p,r,f,_ = precision_recall_fscore_support(gold_labels, \n",
    "                                              predicted_labels,\n",
    "                                              average='macro',\n",
    "                                              zero_division=0)\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "    print(\"F1: \", f)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609a453-6ac2-4eeb-924b-538babf86ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(test_labels, np.argmax(test_predictions.predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5aff6-6ba1-4f49-b27d-55b50ed6dbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
