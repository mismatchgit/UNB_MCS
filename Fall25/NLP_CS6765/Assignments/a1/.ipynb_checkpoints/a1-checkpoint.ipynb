{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fae7d75-e9d4-4145-9ea6-d76acf20630b",
   "metadata": {},
   "source": [
    "# CS4765/6765 Assingment 1: Spelling Correction\n",
    "\n",
    "**Due 29 September**\n",
    "\n",
    "In this assignment you will implement a bigram language model and stupid backoff and apply them for the task of spelling correction. (I've taken care of the task of spelling error detection for you.) You will compare your models with a unigram language model.\n",
    "\n",
    "Read through this notebook in its entirety before getting started. You should only make changes / write code in parts of the notebook where the instructions ask you to do so. These are indicated with TODO throughout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c41fa4-f768-4225-922c-95d6d9de2710",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The starter code you have been provided with handles reading the data and tokenization for you. This description is provided to help you to understand how the starter code works and what it is doing.\n",
    "\n",
    "I've provided you with the following additional files for this assignment: \n",
    "\n",
    "- `dev.txt` This is development data that you can use to evaluate your models before submitting them.\n",
    "\n",
    "  This data is a collection of errors in essays by students, taken from the Holbrook Corpus, a well-known spelling error corpus.\n",
    "\n",
    "  Each line of this file represents a sentence that includes exactly 1 spelling error, which has already been identified for you. Each line of this file consists of an integer, followed by a tab, followed by a sentence. The integer represents the (zero-based) index of the token that has been identified as a spelling error in the sentence. For example, in the following sentence:\n",
    "\n",
    "  `2   <s> in tow minutes she was back as jane came in the door he hit her on the back of the head she fell to the ground </s>`\n",
    "\n",
    "  the token at index 2 (tow) is a spelling error. Your job is to correct the indicated spelling errors.\n",
    "\n",
    "  All of the errors that have been identified for correction, and their corrections, consist entirely of alphabetic characters; additionally, the correction is always within edit distance one of the error. (The spelling error detection is imperfect. If you do find other spelling errors in the dataset, you should not attempt to correct them.)\n",
    "\n",
    "  I have applied a simple (regex-based) tokenizer to the sentences, and have eliminated most punctuation. \n",
    "\n",
    "  For this assignment we will tokenize the sentences based on whitespace. I.e., eliminate the initial number and tab, and then split the remaining string based on single whitespace characters. (If you look closely at the data, you’ll see that this tokenization strategy is imperfect. That’s OK for this assignment. Just treat whatever you get from splitting on whitespace as tokens, even if there are some oddities.) The sentences have already been padded with special tokens marking the beginning and end of sentences (`<s>` and `</s>`).\n",
    "\n",
    "  Note that the starter code you have been provided with handles reading the data and tokenization for you. You should not modify these parts of the starter code.\n",
    "\n",
    "- `dev.keys.txt` This file has one word per line; each line is the correction to the spelling error on the corresponding line in `dev.txt`. You will use this file for evaluating your spelling corrector during development.\n",
    "\n",
    "- `test.txt` and `test.keys.txt` This is test data, taken from the same source as the development data, and in the same format. You will use this data for final evaluation of your spelling corrector.\n",
    "\n",
    "- `corpus.txt` A sample of sentences from the Brown Corpus, tokenized in the same manner as `dev.txt`. You will use this corpus to estimate your language models. (This is a rather small corpus, only about 600<i>k</i> words. In practice you would use a much larger corpus to estimate a language model. However, we’re using a small corpus here to keep the computation manageable.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44376bd1-f7f0-4be9-ae77-e3e1c58fea32",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We will use the following model for spelling correction. For a given misspelling $x$, the system's prediction of the correct spelling, $\\hat{w}$, is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{w} = \\mathrm{argmax}_{w \\in C} P(w) \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "where $P(w)$ is the language model, and $C$ is a set of candidate corrections for the misspelling $x$.\n",
    "\n",
    "\n",
    "### Unigram language model\n",
    "\n",
    "The starter code provides a unigram language model. This model does not use any smoothing. Any word that doesn't occur in the training data has probability 0.\n",
    "\n",
    "### Bigram language model\n",
    "\n",
    "Implement a bigram language model with Laplace smoothing. Treat any unknown word as a single word (e.g., UNK) which appears in the training data with frequency 0. (Be sure to account for this UNK type when determining the size of the vocabulary.)\n",
    "\n",
    "Note that in Equation 1 we write $P(w)$ for the language model. Really, though, we’re interested in the probability of the entire sentence, with $w$ replacing $x$. For a bigram language model, a given word instance participates in two bigrams, one with the word before it and one with the word after it. To take this into consideration, we will compute $P(w)$ as below:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w) ≈ P(w_i|w_{i−1})P(w_{i+1}|w_i) \\textrm{ where here } w \\textrm{ is the word at position } i \\ (\\textrm{i.e.}, w_i).\n",
    "\\end{equation}\n",
    "\n",
    "### Stupid Backoff\n",
    "\n",
    "Implement stupid backoff as described in Section 3.6.4 of the textbook. In our case, we will only consider up to the case of bigrams, and so, following the notation in the textbook, we will implement this as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "S(w_i|w_{i-1}) = \\begin{cases}\n",
    "\\frac{\\textrm{count}(w_{i-1} w_i)}{\\textrm{count}(w_{i-1})} & \\textrm{if count}(w_{i-1} w_i) > 0 \\\\\n",
    "\\lambda \\frac{\\textrm{count}(w_i)}{N} & \\textrm{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of token instances (i.e., similarly to the case of the unigram language model).\n",
    "\n",
    "To use stupid backoff in our spelling corrector, we will compute $P(w)$ as below:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w) ≈ S(w_i|w_{i−1})S(w_{i+1}|w_i) \\textrm{ where here } w \\textrm{ is the word at position } i \\ (\\textrm{i.e.}, w_i).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Candidate Corrections\n",
    "\n",
    "For a given misspelling $x$, the set of candidate corrections is all\n",
    "words that are within edit distance 1 of $x$ and in-vocabulary; the\n",
    "vocabulary here is all words (types) in the training corpus\n",
    "(`corpus.txt`). The edit operations are insertion, deletion,\n",
    "substitution, and transposition.\n",
    "\n",
    "Note that the class `CandidateModel` in the starter code below takes care of determining the set of candidate corrections $C$ for a given misspelling $x$ by enumerating all in-vocabulary words that can be arrived at by applying an edit operation to $x$. (An alternative to find all in-vocabulary words within edit distance 1 of $x$ would be to compute the edit distance between $x$ and each word in the vocabulary; however, this approach would tend to be slower.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ecaa0-5c2b-4767-8e3b-75e720ff7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model of candidate in-vocabulary corrections for a spelling error.\n",
    "# See below for an example of how to use it.\n",
    "class CandidateModel:\n",
    "    def __init__(self, train_corpus_fname):\n",
    "        self.ALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.vocabulary = set()\n",
    "        for line in open(train_corpus_fname):\n",
    "            line = line.split()\n",
    "            # Ignore start and end of sentence markers\n",
    "            words = line[1:len(line) - 1]\n",
    "            for w in words:\n",
    "                self.vocabulary.add(w)\n",
    "\n",
    "    def delete_edits(self, w):\n",
    "        # Return the set of strings that can be formed by applying one\n",
    "        # delete operation to word w\n",
    "        result = []\n",
    "        for i in range(len(w)):\n",
    "            candidate = w[:i] + w[i+1:]\n",
    "            result.append(candidate)\n",
    "        return result\n",
    "\n",
    "    def insert_edits(self, w):\n",
    "        result = []\n",
    "        for i in range(len(w) + 1):\n",
    "            for c in self.ALPHABET:\n",
    "                candidate = w[:i] + c + w[i:]\n",
    "                result.append(candidate)\n",
    "        return result\n",
    "\n",
    "    def transpose_edits(self, w):\n",
    "        result = []\n",
    "        for i in range(1, len(w)):\n",
    "            transposed_letters = w[i] + w[i-1]\n",
    "            candidate = w[:i-1] + transposed_letters + w[i+1:]\n",
    "            result.append(candidate)\n",
    "        return result\n",
    "\n",
    "    def replace_edits(self, w):\n",
    "        result = []\n",
    "        for i in range(len(w)):\n",
    "            for c in self.ALPHABET:\n",
    "                if c != w[i]:\n",
    "                    candidate = w[:i] + c + w[i+1:]\n",
    "                    result.append(candidate)\n",
    "        return result\n",
    "\n",
    "    def candidates(self, w, in_vocabulary=True):\n",
    "        all_candidates = self.delete_edits(w) + self.insert_edits(w) + self.transpose_edits(w) + self.replace_edits(w)\n",
    "        if in_vocabulary:\n",
    "            all_candidates = [x for x in all_candidates if x in self.vocabulary]\n",
    "        return set(all_candidates)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e4b42-2348-4a4f-8042-17045fa7fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = 'data/corpus.txt'\n",
    "candidate_model = CandidateModel(train_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2eebb7-459c-417a-a575-1f44895dea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_model can be used to get the set of in-vocabulary candidate corrections for a spelling error.\n",
    "# All candidate corrections are within edit distance one of the spelling error.\n",
    "candidate_model.candidates('frend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ff0ad-518b-462d-9b6c-8cc4335a338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "EPS = 0.0001\n",
    "\n",
    "# An unsmoothed unigram language model. You should read this to understand how it works. \n",
    "# See examples below of how to use it.\n",
    "class UnsmoothedUnigramLM:\n",
    "    def __init__(self, fname):\n",
    "        self.counts = {}\n",
    "        self.train(fname)\n",
    "\n",
    "    def train(self, fname):\n",
    "        for line in open(fname):\n",
    "            tokens = line.split()\n",
    "            for t in tokens:\n",
    "                self.counts[t] = self.counts.get(t, 0) + 1\n",
    "                \n",
    "        # Computing this sum once during training, instead of every\n",
    "        # time it's needed in log_prob, speeds things up\n",
    "        self.num_instances = sum(self.counts.values())\n",
    "\n",
    "    def log_prob(self, word):\n",
    "        # Compute probabilities in log space to avoid underflow errors\n",
    "        # (This is not actually a problem for this language model, but\n",
    "        # it can become an issue when we multiply together many\n",
    "        # probabilities)\n",
    "        if word in self.counts:\n",
    "            return math.log(self.counts[word]) - math.log(self.num_instances)\n",
    "        else:\n",
    "            # This is a bit of a hack to get a float with the value of\n",
    "            # minus infinity for words that have probability 0\n",
    "            return float(\"-inf\")\n",
    "\n",
    "    # These methods might be helpful later for implementing Stupid Backoff\n",
    "    def get_count(self, word):\n",
    "        return self.counts.get(word, 0)\n",
    "\n",
    "    def get_num_instances(self):\n",
    "        return self.num_instances\n",
    "    \n",
    "    def check_probs(self):\n",
    "        # Hint: Writing code to check whether the probabilities you\n",
    "        # have computed form a valid probability distribution is very\n",
    "        # helpful, particularly when you start incorporating smoothing\n",
    "        # It can be a bit slow, however, especially for bigram language \n",
    "        # models, so you might want to turn these checks off once \n",
    "        # you're convinced things are working correctly.\n",
    "\n",
    "        # Make sure the probability for each word is between 0 and 1\n",
    "        for w in self.counts:\n",
    "            assert 0 - EPS < math.exp(self.log_prob(w)) < 1 + EPS\n",
    "        # Make sure that the sum of probabilities for all words is 1\n",
    "        assert 1 - EPS < \\\n",
    "            sum([math.exp(self.log_prob(w)) for w in self.counts]) < \\\n",
    "            1 + EPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66fb54-4e4d-413b-878a-ec55d056eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement BigramLM following the explanation of the Bigram Language Model above by \n",
    "# completing the constructor and log_prob methods. You are welcome to use additional methods\n",
    "# as needed for your solution, but you should not change the signature for the constructor or \n",
    "# log_prob or check_probs methods because other parts of the starter code (which you shouldn't \n",
    "# modify) rely on these.\n",
    "\n",
    "class BigramLM:\n",
    "    def __init__(self, fname):\n",
    "        # TODO Complete this method\n",
    "        pass\n",
    "        \n",
    "    def log_prob(self, w1, w2):\n",
    "        # TODO Complete this method\n",
    "        # This method should return log(P(w2|w1) (using add-1 smoothing)\n",
    "\n",
    "        # Delete this (it's only here so the starter code runs without errors initially\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    def check_probs(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89f734-38ee-4cac-92f5-b57b3b049110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement Stupid Backoff following the explanation of Stupid Backoff above.\n",
    "# Note that you should have the various counts required to do this in unigram_lm\n",
    "# and bigram_lm. As such, your implementation here should be very short. (My\n",
    "# sample solution is about 4 lines. If you're writing a lot of code, you are \n",
    "# likely off track.)\n",
    "\n",
    "def stupid_backoff_score(w1, w2, lmbda):\n",
    "    # Delete this (it's only here so the starter code runs without errors initially\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7781f0-554a-42e6-bde3-14adfdc7c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the language models\n",
    "unigram_lm = UnsmoothedUnigramLM(train_fname)\n",
    "bigram_lm = BigramLM(train_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d3243-7133-4132-b111-63dbce3cfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can comment out these lines out to run faster...\n",
    "unigram_lm.check_probs()\n",
    "bigram_lm.check_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf3e24-3964-4d2c-866a-f2d813adaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(predict_dataset_fname, model, stupid_backoff_lambda=1):\n",
    "    # Get the predictions for a model for each instance in a dataset\n",
    "    # model must be 1 of 'unigram', 'bigram', 'stupid'\n",
    "    # stupid_backoff_alpha is only used when model = 'stupid'\n",
    "    corrections = []\n",
    "    for line in open(predict_dataset_fname):\n",
    "        # Split the line on a tab; get the target word to correct and\n",
    "        # the sentence it's in\n",
    "        target_index,sentence = line.split('\\t')\n",
    "        target_index = int(target_index)\n",
    "        sentence = sentence.split()\n",
    "        target_word = sentence[target_index]\n",
    "\n",
    "        # Get the in-vocabulary candidates \n",
    "        iv_candidates = candidate_model.candidates(target_word)\n",
    "\n",
    "        # Find the candidate correction with the highest probability;\n",
    "        # if no candidate has non-zero probability, or there are no\n",
    "        # candidates, give up and output the original target word as\n",
    "        # the correction.\n",
    "        best_prob = float('-inf')\n",
    "        best_correction = target_word\n",
    "        for ivc in sorted(iv_candidates):\n",
    "            if model == 'unigram':\n",
    "                unigram_log_prob = unigram_lm.log_prob(ivc)\n",
    "                ivc_log_prob = unigram_log_prob\n",
    "            elif model == 'bigram':\n",
    "                bigram_log_prob = bigram_lm.log_prob(sentence[target_index - 1], ivc)\n",
    "                bigram_log_prob += bigram_lm.log_prob(ivc, sentence[target_index + 1])\n",
    "                ivc_log_prob = bigram_log_prob\n",
    "            elif model == 'stupid':\n",
    "                # Note that for stupid backoff we're not using log space\n",
    "                ivc_log_prob = stupid_backoff_score(sentence[target_index - 1], ivc, stupid_backoff_lambda)\n",
    "                ivc_log_prob *= stupid_backoff_score(ivc, sentence[target_index + 1], stupid_backoff_lambda)         \n",
    "            else:\n",
    "                assert False\n",
    "            if ivc_log_prob > best_prob:\n",
    "                best_prob = ivc_log_prob\n",
    "                best_correction = ivc\n",
    "        corrections.append(best_correction)\n",
    "    return corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0050a8b-90a2-4afd-91ca-76a58da0320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_for_predictions(predictions, keys):\n",
    "    # If the length of the output and keys are not the same, something went\n",
    "    # wrong...\n",
    "    assert len(predictions) == len(keys)\n",
    "\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    for p,k in zip(predictions,keys):\n",
    "        if p == k:\n",
    "            num_correct += 1\n",
    "        total += 1\n",
    "    accuracy = num_correct / total\n",
    "    print(\"Num correct: \", num_correct)\n",
    "    print(\"Total: \", total)\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123ec65-83ee-49d9-b359-2c1b3644d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_fname = 'data/dev.txt'\n",
    "unigram_dev_predictions = get_predictions(dev_fname, 'unigram')\n",
    "bigram_dev_predictions = get_predictions(dev_fname, 'bigram')\n",
    "# The third argument to get_predictions is the weight for stupid backoff \n",
    "# (lambda in Eqation 3.31 in the textbook)\n",
    "stupid_dev_predictions = get_predictions(dev_fname, 'stupid', 1)\n",
    "\n",
    "dev_keys_fname = 'data/dev.keys.txt'\n",
    "dev_keys = [x.strip() for x in open(dev_keys_fname)]\n",
    "\n",
    "print('Unigram:')\n",
    "print_accuracy_for_predictions(unigram_dev_predictions, dev_keys)\n",
    "\n",
    "print()\n",
    "print('Bigram:')\n",
    "print_accuracy_for_predictions(bigram_dev_predictions, dev_keys)\n",
    "\n",
    "print()\n",
    "print('Stupid backoff:')\n",
    "print_accuracy_for_predictions(stupid_dev_predictions, dev_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd60fa3-7a82-4272-a7a9-666d31832440",
   "metadata": {},
   "source": [
    "The code above uses stupid backoff with a single value for lambda (1). Your task here is to find a good value for lambda. We will discuss approaches for doing this in lecture. Importantly, you must only consider the development data (and crucially not the test data) when doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef058ca4-549a-4ad3-b801-13082dbfe07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here to find a good value for lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afa773-835d-4a55-aad6-2ccc27758982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done all of the parts above, evaluate the models on the test data by running this cell.\n",
    "# Note that you should set the value of BEST_LAMBDA to whatever value you selected over the dev data above.\n",
    "test_fname = 'data/test.txt'\n",
    "unigram_test_predictions = get_predictions(test_fname, 'unigram')\n",
    "bigram_test_predictions = get_predictions(test_fname, 'bigram')\n",
    "\n",
    "BEST_LAMBDA = 1\n",
    "stupid_test_predictions = get_predictions(test_fname, 'stupid', BEST_LAMBDA)\n",
    "\n",
    "test_keys_fname = 'data/test.keys.txt'\n",
    "test_keys = [x.strip() for x in open(test_keys_fname)]\n",
    "\n",
    "print('Unigram:')\n",
    "print_accuracy_for_predictions(unigram_test_predictions, test_keys)\n",
    "\n",
    "print()\n",
    "print('Bigram:')\n",
    "print_accuracy_for_predictions(bigram_test_predictions, test_keys)\n",
    "\n",
    "print()\n",
    "print('Stupid backoff:')\n",
    "print_accuracy_for_predictions(stupid_test_predictions, test_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc43eb-a888-4fa2-bce1-0b353e2a60bd",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "Write a report addressing at least the following points:\n",
    "\n",
    "1. Compare the 3 models (unigram, bigram, and stupid backoff). Which model performs best? Is the relative performance of the models consistent across the development and test data?\n",
    "\n",
    "1. For whichever model you find to perform best, why do you believe it performs better than the others? Justify your answer?\n",
    "\n",
    "1. Clearly describe the process that you used to select the best value for lambda. What value of lambda did you find to work best?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206b0af-9098-423e-88a8-1b95227c6a62",
   "metadata": {},
   "source": [
    "**TODO** Write your report as markdown in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96acf6e8-324c-4968-9a0e-683ac38fa144",
   "metadata": {},
   "source": [
    "## What to submit\n",
    "\n",
    "When you're done, submit this file to the assignment 1 dropbox on D2L. (You don't need to submit any of the data files we provided you with for this assignment).\n",
    "\n",
    "## Grading\n",
    "\n",
    "Your assignments will be graded based primarily on the correctness of their implementation and the written answers in the report.\n",
    "\n",
    "Assignments that do not conform to the specifications outlined above might not be graded (e.g., modifying parts of the starter code that you were not asked to modify). Assignments that we are unable to run in a reasonable amount of time (less than one minute) also might not be graded. Grades will be out of 10 and broken down as follows:\n",
    "\n",
    "- Bigram LM: 5\n",
    "\n",
    "- Stupid Backoff: 2\n",
    "\n",
    "- Report / discussion: 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
